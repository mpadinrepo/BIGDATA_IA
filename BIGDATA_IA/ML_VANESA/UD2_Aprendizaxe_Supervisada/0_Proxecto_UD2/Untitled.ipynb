{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c94223-d5b3-4ddd-bee4-660b3feec5c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Comencemos con la primera etapa: la importación de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53beb845-0008-4467-83af-7eedb5e76b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas de dataframeTrain:\n",
      "   age          job  marital          education  default housing loan  \\\n",
      "0   44  blue-collar  married           basic.4y  unknown     yes   no   \n",
      "1   53   technician  married            unknown       no      no   no   \n",
      "2   28   management   single  university.degree       no     yes   no   \n",
      "3   39     services  married        high.school       no      no   no   \n",
      "4   55      retired  married           basic.4y       no     yes   no   \n",
      "\n",
      "    contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
      "0  cellular   aug         thu  ...         1    999         0  nonexistent   \n",
      "1  cellular   nov         fri  ...         1    999         0  nonexistent   \n",
      "2  cellular   jun         thu  ...         3      6         2      success   \n",
      "3  cellular   apr         fri  ...         2    999         0  nonexistent   \n",
      "4  cellular   aug         fri  ...         1      3         1      success   \n",
      "\n",
      "  emp_var_rate  cons_price_idx  cons_conf_idx  euribor3m  nr_employed  y  \n",
      "0          1.4          93.444          -36.1      4.963       5228.1  0  \n",
      "1         -0.1          93.200          -42.0      4.021       5195.8  0  \n",
      "2         -1.7          94.055          -39.8      0.729       4991.6  1  \n",
      "3         -1.8          93.075          -47.1      1.405       5099.1  0  \n",
      "4         -2.9          92.201          -31.4      0.869       5076.2  1  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Información del conjunto de datos de entrenamiento:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp_var_rate    41188 non-null  float64\n",
      " 16  cons_price_idx  41188 non-null  float64\n",
      " 17  cons_conf_idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr_employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  int64  \n",
      "dtypes: float64(5), int64(6), object(10)\n",
      "memory usage: 6.6+ MB\n",
      "None\n",
      "\n",
      "Primeras filas de dataframeTest:\n",
      "   age          job  marital          education default  housing     loan  \\\n",
      "0   30  blue-collar  married           basic.9y      no      yes       no   \n",
      "1   39     services   single        high.school      no       no       no   \n",
      "2   25     services  married        high.school      no      yes       no   \n",
      "3   38     services  married           basic.9y      no  unknown  unknown   \n",
      "4   47       admin.  married  university.degree      no      yes       no   \n",
      "\n",
      "     contact month day_of_week  duration  campaign  pdays  previous  \\\n",
      "0   cellular   may         fri       487         2    999         0   \n",
      "1  telephone   may         fri       346         4    999         0   \n",
      "2  telephone   jun         wed       227         1    999         0   \n",
      "3  telephone   jun         fri        17         3    999         0   \n",
      "4   cellular   nov         mon        58         1    999         0   \n",
      "\n",
      "      poutcome  emp_var_rate  cons_price_idx  cons_conf_idx  euribor3m  \\\n",
      "0  nonexistent          -1.8          92.893          -46.2      1.313   \n",
      "1  nonexistent           1.1          93.994          -36.4      4.855   \n",
      "2  nonexistent           1.4          94.465          -41.8      4.962   \n",
      "3  nonexistent           1.4          94.465          -41.8      4.959   \n",
      "4  nonexistent          -0.1          93.200          -42.0      4.191   \n",
      "\n",
      "   nr_employed  \n",
      "0       5099.1  \n",
      "1       5191.0  \n",
      "2       5228.1  \n",
      "3       5228.1  \n",
      "4       5195.8  \n",
      "\n",
      "Información del conjunto de datos de prueba:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4119 entries, 0 to 4118\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             4119 non-null   int64  \n",
      " 1   job             4119 non-null   object \n",
      " 2   marital         4119 non-null   object \n",
      " 3   education       4119 non-null   object \n",
      " 4   default         4119 non-null   object \n",
      " 5   housing         4119 non-null   object \n",
      " 6   loan            4119 non-null   object \n",
      " 7   contact         4119 non-null   object \n",
      " 8   month           4119 non-null   object \n",
      " 9   day_of_week     4119 non-null   object \n",
      " 10  duration        4119 non-null   int64  \n",
      " 11  campaign        4119 non-null   int64  \n",
      " 12  pdays           4119 non-null   int64  \n",
      " 13  previous        4119 non-null   int64  \n",
      " 14  poutcome        4119 non-null   object \n",
      " 15  emp_var_rate    4119 non-null   float64\n",
      " 16  cons_price_idx  4119 non-null   float64\n",
      " 17  cons_conf_idx   4119 non-null   float64\n",
      " 18  euribor3m       4119 non-null   float64\n",
      " 19  nr_employed     4119 non-null   float64\n",
      "dtypes: float64(5), int64(5), object(10)\n",
      "memory usage: 643.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importar los conjuntos de datos\n",
    "dataframeTrain = pd.read_csv(\"credit-train.csv\")\n",
    "dataframeTest = pd.read_csv(\"credit-test.csv\")\n",
    "\n",
    "# Mostrar las primeras filas y la información del conjunto de datos de entrenamiento\n",
    "print(\"Primeras filas de dataframeTrain:\")\n",
    "print(dataframeTrain.head())\n",
    "print(\"\\nInformación del conjunto de datos de entrenamiento:\")\n",
    "print(dataframeTrain.info())\n",
    "\n",
    "# Mostrar las primeras filas y la información del conjunto de datos de prueba\n",
    "print(\"\\nPrimeras filas de dataframeTest:\")\n",
    "print(dataframeTest.head())\n",
    "print(\"\\nInformación del conjunto de datos de prueba:\")\n",
    "print(dataframeTest.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc6ea4-5a73-482c-a457-3080a212968e",
   "metadata": {},
   "source": [
    " etapa 2 realizaremos un análisis numérico y visual de los datos. Comenzaremos por explorar las características numéricas en nuestros conjuntos de datos. Utilizaremos estadísticas descriptivas para comprender la distribución y las tendencias de estas características.\n",
    "\n",
    "A continuación, analizaremos las características categóricas mediante gráficos de barras para comprender la distribución de las diferentes categorías en cada característica.\n",
    "\n",
    "Comencemos con el análisis numérico.\n",
    "\n",
    "Para el análisis numérico, calcularemos algunas estadísticas descriptivas para las características numéricas en nuestros conjuntos de datos, como la media, la mediana, la desviación estándar, el mínimo y el máximo. Esto nos dará una idea de la distribución y la dispersión de estos datos.\n",
    "\n",
    "Veamos las estadísticas descriptivas para las características numéricas en el conjunto de datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f973adf-84d6-4055-bc9a-54f3f8d1c522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadísticas descriptivas para características numéricas en el conjunto de datos de entrenamiento:\n",
      "               age      duration      campaign         pdays      previous  \\\n",
      "count  41188.00000  41188.000000  41188.000000  41188.000000  41188.000000   \n",
      "mean      40.02406    258.285010      2.567593    962.475454      0.172963   \n",
      "std       10.42125    259.279249      2.770014    186.910907      0.494901   \n",
      "min       17.00000      0.000000      1.000000      0.000000      0.000000   \n",
      "25%       32.00000    102.000000      1.000000    999.000000      0.000000   \n",
      "50%       38.00000    180.000000      2.000000    999.000000      0.000000   \n",
      "75%       47.00000    319.000000      3.000000    999.000000      0.000000   \n",
      "max       98.00000   4918.000000     56.000000    999.000000      7.000000   \n",
      "\n",
      "       emp_var_rate  cons_price_idx  cons_conf_idx     euribor3m  \\\n",
      "count  41188.000000    41188.000000   41188.000000  41188.000000   \n",
      "mean       0.081886       93.575664     -40.502600      3.621291   \n",
      "std        1.570960        0.578840       4.628198      1.734447   \n",
      "min       -3.400000       92.201000     -50.800000      0.634000   \n",
      "25%       -1.800000       93.075000     -42.700000      1.344000   \n",
      "50%        1.100000       93.749000     -41.800000      4.857000   \n",
      "75%        1.400000       93.994000     -36.400000      4.961000   \n",
      "max        1.400000       94.767000     -26.900000      5.045000   \n",
      "\n",
      "        nr_employed             y  \n",
      "count  41188.000000  41188.000000  \n",
      "mean    5167.035911      0.112654  \n",
      "std       72.251528      0.316173  \n",
      "min     4963.600000      0.000000  \n",
      "25%     5099.100000      0.000000  \n",
      "50%     5191.000000      0.000000  \n",
      "75%     5228.100000      0.000000  \n",
      "max     5228.100000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Estadísticas descriptivas para características numéricas en el conjunto de datos de entrenamiento\n",
    "numeric_features_train = dataframeTrain.select_dtypes(include=['int64', 'float64'])\n",
    "numeric_descriptions_train = numeric_features_train.describe()\n",
    "print(\"Estadísticas descriptivas para características numéricas en el conjunto de datos de entrenamiento:\")\n",
    "print(numeric_descriptions_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883e238-bcad-4bcb-97b1-0b2d3635b4fd",
   "metadata": {},
   "source": [
    "# Las estadísticas descriptivas para las características numéricas en el conjunto de datos de entrenamiento son las siguientes:\r\n",
    "\r\n",
    "age: La edad promedio de los clientes es de aproximadamente 40 años, con una desviación estándar de aproximadamente 10.42. La edad mínima es 17 y la máxima es 9\n",
    "8.\r\n",
    "duration: La duración promedio de las llamadas es de alrededor de 258 segundos, con una desviación estándar de aproximadamente 259. La duración mínima es 0 segundos y la máxima es 4918 segund\n",
    "os.\r\n",
    "campaign: El número promedio de contactos durante la campaña es de alrededor de 2.57, con una desviación estándar de aproximadamente 2.77. El número mínimo de contactos es 1 y el máximo es\n",
    " 56.\r\n",
    "pdays: El número promedio de días que han pasado desde el último contacto es de aproximadamente 962.48, con una desviación estándar de aproximadamente 186.91. El valor mínimo es 0 y el máximo es\n",
    " 999.\r\n",
    "previous: El número promedio de contactos anteriores a esta campaña es de aproximadamente 0.17, con una desviación estándar de aproximadamente 0.49. El valor mínimo es 0 y el máximo\n",
    " es 7.\r\n",
    "emp_var_rate: La tasa de variación del empleo tiene una media de aproximadamente 0.08, con una desviación estándar de aproximadamente 1.57. El valor mínimo es -3.4 y el máximo \n",
    "es 1.4.\r\n",
    "cons_price_idx: El índice de precios al consumidor tiene una media de aproximadamente 93.58, con una desviación estándar de aproximadamente 0.58. El valor mínimo es 92.201 y el máximo es\n",
    " 94.767.\r\n",
    "cons_conf_idx: El índice de confianza del consumidor tiene una media de aproximadamente -40.5, con una desviación estándar de aproximadamente 4.63. El valor mínimo es -50.8 y el máximo \n",
    "es -26.9.\r\n",
    "euribor3m: La tasa Euribor a 3 meses tiene una media de aproximadamente 3.62, con una desviación estándar de aproximadamente 1.73. El valor mínimo es 0.634 y el máximo\n",
    " es 5.045.\r\n",
    "nr_employed: El número de empleados tiene una media de aproximadamente 5167.04, con una desviación estándar de aproximadamente 72.25. El valor mínimo es 4963.6 y el máximo\n",
    " es 5228.1.\r\n",
    "y: La columna objetivo tiene un promedio de alrededor de 0.11, lo que indica que aproximadamente el 11.27% de los clientes se suscribieron al crédito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c2db4d-bb2d-4b45-8056-9ff376d2d715",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Etapa 3: Análisis y Visualización de Datos\n",
    "En esta etapa, realizaremos un análisis y visualización de los datos para comprender mejor las relaciones entre las características y la variable objetivo. Esto nos ayudará a identificar patrones y tendencias en los datos que pueden ser útiles para el modelado posterior.\n",
    "\n",
    "Análisis de Variables Categóricas\n",
    "Comencemos analizando las variables categóricas en nuestros conjuntos de datos. Calcularemos la frecuencia de cada categoría y visualizaremos estas distribuciones.\n",
    "\n",
    "Vamos a proceder con este análisis.\n",
    "\n",
    "Para analizar las variables categóricas en nuestros conjuntos de datos, podemos usar diferentes métodos dependiendo de la naturaleza de los datos. Algunas opciones incluyen:\n",
    "\n",
    "Conteo de valores únicos: Podemos contar la cantidad de valores únicos para cada categoría en cada variable categórica.\n",
    "Gráficos de barras: Podemos visualizar la distribución de las categorías utilizando gráficos de barras.\n",
    "Tablas de contingencia: Podemos crear tablas de contingencia para analizar la relación entre diferentes variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69747b64-4c75-40b0-8e1c-5aae62adb5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas categóricas en el conjunto de datos de entrenamiento:\n",
      "['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
      "\n",
      "Columnas categóricas en el conjunto de datos de prueba:\n",
      "['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identificar las columnas categóricas en el conjunto de datos de entrenamiento\n",
    "categorical_cols_train = dataframeTrain.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Columnas categóricas en el conjunto de datos de entrenamiento:\")\n",
    "print(categorical_cols_train)\n",
    "\n",
    "# Identificar las columnas categóricas en el conjunto de datos de prueba\n",
    "categorical_cols_test = dataframeTest.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nColumnas categóricas en el conjunto de datos de prueba:\")\n",
    "print(categorical_cols_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67dd40-ad57-4723-95f4-a084d4e836c0",
   "metadata": {},
   "source": [
    "hemos identificado las columnas categóricas en ambos conjuntos de datos, podemos decidir cómo codificarlas. Hay varias opciones, como la codificación one-hot, la codificación ordinal o la codificación basada en frecuencias.\n",
    "\n",
    "Dado que todas las columnas categóricas tienen un número razonable de categorías y no hay un orden inherente en las categorías, podemos optar por la codificación one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10346da-be0f-4b1b-89f8-9c05049d592b",
   "metadata": {},
   "source": [
    "Este código creará nuevas columnas para cada categoría en las columnas categóricas originales, utilizando la codificación one-hot. Luego, mostrará las primeras filas del conjunto de datos de entrenamiento codificado para que puedas verificar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389ba469-5606-4877-92ac-72484e40f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de datos de entrenamiento codificado:\n",
      "   age  duration  campaign  pdays  previous  emp_var_rate  cons_price_idx  \\\n",
      "0   44       210         1    999         0           1.4          93.444   \n",
      "1   53       138         1    999         0          -0.1          93.200   \n",
      "2   28       339         3      6         2          -1.7          94.055   \n",
      "3   39       185         2    999         0          -1.8          93.075   \n",
      "4   55       137         1      3         1          -2.9          92.201   \n",
      "\n",
      "   cons_conf_idx  euribor3m  nr_employed  ...  month_oct  month_sep  \\\n",
      "0          -36.1      4.963       5228.1  ...      False      False   \n",
      "1          -42.0      4.021       5195.8  ...      False      False   \n",
      "2          -39.8      0.729       4991.6  ...      False      False   \n",
      "3          -47.1      1.405       5099.1  ...      False      False   \n",
      "4          -31.4      0.869       5076.2  ...      False      False   \n",
      "\n",
      "   day_of_week_fri  day_of_week_mon  day_of_week_thu  day_of_week_tue  \\\n",
      "0            False            False             True            False   \n",
      "1             True            False            False            False   \n",
      "2            False            False             True            False   \n",
      "3             True            False            False            False   \n",
      "4             True            False            False            False   \n",
      "\n",
      "   day_of_week_wed  poutcome_failure  poutcome_nonexistent  poutcome_success  \n",
      "0            False             False                  True             False  \n",
      "1            False             False                  True             False  \n",
      "2            False             False                 False              True  \n",
      "3            False             False                  True             False  \n",
      "4            False             False                 False              True  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# Aplicar codificación one-hot a las columnas categóricas en el conjunto de datos de entrenamiento\n",
    "dataframeTrain_encoded = pd.get_dummies(dataframeTrain, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "\n",
    "# Mostrar las primeras filas del conjunto de datos de entrenamiento codificado\n",
    "print(\"Conjunto de datos de entrenamiento codificado:\")\n",
    "print(dataframeTrain_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3cd5c7-9212-4680-92b7-1615ae5ddd75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Codificación one-hot de las columnas categóricas en el conjunto de datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ccd71e-658f-452a-b673-0c332ce44159",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorical_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Aplicar la codificación one-hot a las columnas categóricas en el conjunto de datos de prueba\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dataframeTest_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(dataframeTest[\u001b[43mcategorical_cols\u001b[49m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convertir la matriz dispersa resultante a un DataFrame de pandas\u001b[39;00m\n\u001b[1;32m     11\u001b[0m dataframeTest_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dataframeTest_encoded\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_cols))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categorical_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Importar la función necesaria\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Crear un objeto OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Aplicar la codificación one-hot a las columnas categóricas en el conjunto de datos de prueba\n",
    "dataframeTest_encoded = encoder.fit_transform(dataframeTest[categorical_cols])\n",
    "\n",
    "# Convertir la matriz dispersa resultante a un DataFrame de pandas\n",
    "dataframeTest_encoded = pd.DataFrame(dataframeTest_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenar el DataFrame codificado con el resto de las columnas del conjunto de datos de prueba\n",
    "dataframeTest_encoded = pd.concat([dataframeTest.drop(columns=categorical_cols), dataframeTest_encoded], axis=1)\n",
    "\n",
    "# Mostrar las primeras filas del conjunto de datos de prueba codificado\n",
    "print(\"Conjunto de datos de prueba codificado:\")\n",
    "print(dataframeTest_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6e497-48ab-4206-8c57-53a8ade3ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "dataframeTest_encoded = pd.get_dummies(dataframeTest, columns=categorical_cols)\n",
    "\n",
    "# Mostrar las primeras filas del conjunto de datos de prueba codificado\n",
    "print(\"Conjunto de datos de prueba codificado:\")\n",
    "print(dataframeTest_encoded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e711f93-ea55-4a9b-ad47-0b8653fb3acc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Calcular métricas de evaluación adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9feb8f0-c92a-4838-9ef9-06cf28f61dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "y_pred_test = modelo.predict(X_test)\n",
    "\n",
    "# Calcular métricas de evaluación adicionales\n",
    "report = classification_report(y_test, y_pred_test)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(report)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f7624-9de4-41cf-addf-308bd4b9b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "y_pred_test = modelo.predict(X_test)  # Reemplaza 'log_reg_model' con el nombre de tu modelo\n",
    "\n",
    "# Calcular métricas de evaluación adicionales\n",
    "report = classification_report(y_test, y_pred_test)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(report)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2506ae6-24cb-47ff-931b-d359ee3bc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dividir los datos en características (X) y variable objetivo (y) para el conjunto de entrenamiento\n",
    "X_train = dataframeTrain_encoded.drop(columns=['y'])\n",
    "y_train = dataframeTrain_encoded['y']\n",
    "\n",
    "# Dividir los datos en características (X) y variable objetivo (y) para el conjunto de prueba\n",
    "X_test = dataframeTest_encoded\n",
    "y_test = dataframeTest_encoded['y']  # Asignar solo la columna 'y'\n",
    "\n",
    "# Dividir los datos de entrenamiento en conjuntos de entrenamiento y validación\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar el modelo de Regresión Logística\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento divididos\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Asegurarnos de que las características en X_test estén en el mismo orden que en X_train\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión del modelo en el conjunto de prueba\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Precisión del modelo en el conjunto de prueba:\", accuracy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce658a-a2e9-43e4-b22e-4e73e616c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b9628-d749-4590-a5a8-9eb27316efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dividir los datos en características (X) y variable objetivo (y) para el conjunto de entrenamiento\n",
    "X_train = dataframeTrain_encoded.drop(columns=['y'])\n",
    "y_train = dataframeTrain_encoded['y']\n",
    "\n",
    "# Dividir los datos en características (X) y variable objetivo (y) para el conjunto de prueba\n",
    "X_test = dataframeTest_encoded.drop(columns=['y'])\n",
    "y_test = dataframeTest_encoded['y']  # Asignar solo la columna 'y'\n",
    "\n",
    "# Asegurarnos de que las columnas en X_train y X_test estén en el mismo orden\n",
    "common_columns = X_train.columns.intersection(X_test.columns)\n",
    "X_train = X_train.reindex(columns=common_columns)\n",
    "X_test = X_test.reindex(columns=common_columns)\n",
    "\n",
    "# Dividir los datos de entrenamiento en conjuntos de entrenamiento y validación\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar el modelo de Regresión Logística con un mayor número máximo de iteraciones\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento divididos\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Convertir los valores de y_test a binarios\n",
    "y_test_binary = y_test.replace({'yes': 1, 'no': 0})\n",
    "\n",
    "# Calcular la precisión del modelo en el conjunto de prueba\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "print(\"Precisión del modelo en el conjunto de prueba:\", accuracy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e4255-b343-4e39-a9aa-24aba1dc66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar y_test para eliminar las filas con valores 'unknown'\n",
    "y_test_filtered = y_test[y_test.isin(['yes', 'no'])]\n",
    "\n",
    "# Convertir los valores de y_test_filtered a binarios\n",
    "y_test_binary = y_test_filtered.replace({'yes': 1, 'no': 0})\n",
    "\n",
    "# Calcular la precisión del modelo en el conjunto de prueba\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "print(\"Precisión del modelo en el conjunto de prueba:\", accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdbdc8-3cd1-4882-88a7-11a918efd241",
   "metadata": {},
   "source": [
    "## MODELO 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fb398-c389-4245-a20c-020abf909676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dividir los datos en características (X) y variable objetivo (y) para el conjunto de entrenamiento\n",
    "X_train = dataframeTrain_encoded.drop(columns=['y'])\n",
    "y_train = dataframeTrain_encoded['y']\n",
    "\n",
    "# Dividir los datos de entrenamiento en conjuntos de entrenamiento y validación\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar el modelo de Regresión Logística con un mayor número máximo de iteraciones\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento divididos\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predecir las etiquetas para los datos de validación\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Calcular la precisión del modelo en el conjunto de validación\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(\"Precisión del modelo en el conjunto de validación:\", accuracy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3888971-b70c-455f-8b8b-c4c3354992f8",
   "metadata": {},
   "source": [
    "coeficientes de las características ordenadas de mayor a menor importancia según su influencia en la predicción. Los coeficientes positivos indican que un aumento en esa característica aumenta la probabilidad de que la clase objetivo sea '1' (por ejemplo, 'sí'), mientras que los coeficientes negativos indican lo contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90db2bc-f565-42b4-8cfe-4c39a73c7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los coeficientes del modelo\n",
    "coefficients = pd.DataFrame(data=model.coef_.reshape(-1, 1), index=X_train.columns, columns=['Coefficient'])\n",
    "coefficients.sort_values(by='Coefficient', ascending=False, inplace=True)\n",
    "\n",
    "# Mostrar los coeficientes más importantes\n",
    "print(\"Coeficientes más importantes:\")\n",
    "print(coefficients.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914173dc-41ed-4e72-9791-0c0fb050c37c",
   "metadata": {},
   "source": [
    "estos coeficientes nos proporcionan información valiosa sobre las características que más influyen en la predicción del modelo. Esta comprensión puede ser útil para mejorar aún más el modelo y optimizar las estrategias de marketing dirigidas a los clientes. \n",
    "\n",
    "month_mar (Mes de Marzo): Parece que el mes de marzo tiene una influencia significativa en la predicción positiva de la variable objetivo. Esto puede deberse a que marzo es un mes en el que se realizan acciones especiales o promociones que pueden aumentar la probabilidad de que un cliente acepte la oferta.\r\n",
    "\r\n",
    "cons_price_idx (Índice de Precios al Consumidor): El índice de precios al consumidor también tiene un impacto positivo en la predicción. Esto sugiere que un mayor índice de precios al consumidor está asociado con una mayor probabilidad de que los clientes acepten la oferta.\r\n",
    "\r\n",
    "default_no (No Incumplimiento de Pagos): La ausencia de incumplimiento de pagos (default_no) también tiene un efecto positivo en la predicción, lo cual es lógico ya que los clientes sin historial de incumplimiento son más propensos a aceptar la oferta.\r\n",
    "\r\n",
    "contact_cellular (Contacto por Teléfono Celular): El contacto por teléfono celular también tiene una influencia positiva en la predicción, lo que sugiere que este método de contacto puede ser más efectivo para persuadir a los clientes.\r\n",
    "\r\n",
    "job_student (Trabajo como Estudiante) y job_retired (Trabajo como Jubilado): Los trabajos como estudiante y jubilado también tienen una influencia positiva en la predicción, lo cual puede deberse a características específicas de estos grupos demográficos que los hacen más propensos a aceptar la preguntar.preguntar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e28542-44a2-437e-ac36-b3784158e358",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# realizar un análisis de residuos en el contexto de la Regresión Logística:\n",
    "\n",
    "Obtención de las probabilidades pronosticadas: Utilizamos el modelo de Regresión Logística para predecir las probabilidades de pertenencia a cada clase para los datos de entrenamiento.\n",
    "\n",
    "Cálculo de los residuos: Calculamos los residuos como la diferencia entre las probabilidades pronosticadas y las respuestas observadas (0 o 1 en el caso de la Regresión Logística).\n",
    "\n",
    "Análisis de los residuos: Visualizamos los residuos para detectar patrones sistemáticos. Esto puede hacerse mediante un gráfico de residuos versus las características relevantes o mediante gráficos como el histograma de residuos.\n",
    "\n",
    "Prueba de homocedasticidad: Comprobamos si la varianza de los residuos es constante en todo el rango de valores pronosticados. Esto puede hacerse mediante un gráfico de residuos versus los valores pronosticados.\n",
    "\n",
    "Prueba de normalidad: Verificamos si los residuos siguen una distribución normal. Esto se puede hacer mediante un gráfico Q-Q (quantile-quantile) o una prueba estadística como la prueba de Shapiro-Wilk.\n",
    "\n",
    "Identificación de valores atípicos: Buscamos valores atípicos o influencia excesiva mediante la identificación de observaciones con residuos altos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896024be-e0b7-4bc9-b3f6-2606110768e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suponiendo que \"y_train_pred\" son las probabilidades pronosticadas por el modelo\n",
    "y_train_pred = model.predict_proba(X_train_split)[:, 1]  # Probabilidad de pertenencia a la clase positiva\n",
    "\n",
    "# Calcular los residuos como la diferencia entre las probabilidades pronosticadas y las respuestas observadas\n",
    "residuos = y_train_split - y_train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac232323-3afd-460f-9564-7bd8b7dc356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de dispersión de residuos versus valores pronosticados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train_pred, residuos, alpha=0.5)\n",
    "plt.xlabel('Probabilidades pronosticadas')\n",
    "plt.ylabel('Residuos')\n",
    "plt.title('Gráfico de Residuos vs. Probabilidades Pronosticadas')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb79c2-e789-42c3-93a1-28c635cafaca",
   "metadata": {},
   "source": [
    "Este gráfico nos ayudará a identificar si hay alguna relación entre los residuos y las probabilidades pronosticadas. Queremos ver una dispersión aleatoria alrededor de la línea cero (la línea punteada roja). Si hay algún patrón, como una curva en forma de U o un patrón en zigzag, podría indicar que el modelo tiene problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed68dc5-e715-4671-b442-9dbdf95e6db2",
   "metadata": {},
   "source": [
    "Hacer un histograma de los residuos para evaluar su distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ede22-1684-4477-8cdf-40fcc26864b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de los residuos\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuos, bins=30, edgecolor='black')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Histograma de Residuos')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b886a-0e3d-4989-b45a-76465b20c571",
   "metadata": {},
   "source": [
    "En este histograma, esperamos ver una distribución simétrica alrededor de cero, indicando que los residuos están distribuidos normalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6d14b-7988-4041-9f7a-0aaf72f78c2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# explorar más a fondo la distribución de los residuos y realizar algunas pruebas estadísticas para evaluar su normalidad.\n",
    "\n",
    "Primero, vamos a importar las bibliotecas necesarias: \n",
    "import seaborn as sns\r\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d6300-6979-48a7-9b07-8e607acd3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369db4d-99bc-4cb6-9e78-1c148bbe9bd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Eliminar columnas no numéricas si es necesario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab555c-f4c8-459b-9753-39200a1948d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas no numéricas si es necesario\n",
    "dataframeTrain_numeric = dataframeTrain_encoded.select_dtypes(include=[np.number])\n",
    "\n",
    "# Identificar valores atípicos usando el método de Tukey\n",
    "Q1 = dataframeTrain_numeric.quantile(0.25)\n",
    "Q3 = dataframeTrain_numeric.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = ((dataframeTrain_numeric < lower_bound) | (dataframeTrain_numeric > upper_bound)).any(axis=1)\n",
    "\n",
    "# Eliminar valores atípicos\n",
    "dataframeTrain_clean = dataframeTrain_encoded[~outliers]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187ce9e-9819-4e2c-ae89-a8e02bceab39",
   "metadata": {},
   "source": [
    "# Prueba de normalidad (Shapiro-Wilk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56243fa-8052-48f8-baa9-cd52dcb60c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stat, p_value = stats.shapiro(residuos)\n",
    "print(\"Estadístico de la prueba:\", stat)\n",
    "print(\"Valor p:\", p_value)\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value > alpha:\n",
    "    print(\"No se rechaza la hipótesis nula: los residuos parecen provenir de una distribución normal.\")\n",
    "else:\n",
    "    print(\"Se rechaza la hipótesis nula: los residuos no parecen provenir de una distribución normal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad425315-cb8b-4eab-a81a-76bda9d72ffe",
   "metadata": {},
   "source": [
    "Si el valor p es mayor que el nivel de significancia (generalmente 0.05), no podemos rechazar la hipótesis nula y podemos decir que los residuos se distribuyen normalmente. De lo contrario, si el valor p es menor que el nivel de significancia, rechazamos la hipótesis nula y concluimos que los residuos no se distribuyen normalmente.\n",
    "\n",
    "Este análisis nos proporciona información importante sobre la calidad del modelo y puede ayudarnos a identificar áreas de mejora. Si los residuos no se distribuyen normalmente o muestran algún patrón sistemático, podríamos necesitar revisar el modelo y considerar ajustes adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461cb3f-ac55-457a-92b4-1c40e0c26903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir la cuadrícula de valores para los hiperparámetros\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],  # Tipo de penalización (L1 o L2)\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # Parámetro de regularización\n",
    "}\n",
    "\n",
    "# Inicializar el modelo de Regresión Logística\n",
    "model = LogisticRegression(max_iter=100000)\n",
    "\n",
    "# Inicializar el GridSearchCV con el modelo y la cuadrícula de parámetros\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Ajustar el GridSearchCV a los datos de entrenamiento\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Mostrar la mejor puntuación (exactitud) obtenida durante la búsqueda\n",
    "print(\"Mejor puntuación (exactitud):\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a4bae-282a-4083-813b-b3dd0e1a90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Definir la cuadrícula de valores para los hiperparámetros\n",
    "param_grid = {\n",
    "    'penalty': ['l2'],  # Cambiamos 'l1' por 'l2'\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # Parámetro de regularización\n",
    "}\n",
    "\n",
    "# Inicializar el modelo de Regresión Logística\n",
    "model = LogisticRegression(max_iter=100000, solver='lbfgs')  # Especificamos el solver\n",
    "\n",
    "# Inicializar el GridSearchCV con el modelo y la cuadrícula de parámetros\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Ajustar el GridSearchCV a los datos de entrenamiento\n",
    "grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Mostrar la mejor puntuación (exactitud) obtenida durante la búsqueda\n",
    "print(\"Mejor puntuación (exactitud):\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4b5de-3901-4a51-94b6-14deb41e6c15",
   "metadata": {},
   "source": [
    "encontrado los mejores hiperparámetros para tu modelo de Regresión Logística:\n",
    "\n",
    "Valor de C: 1\n",
    "Tipo de penalización: L2 (Ridge)\n",
    "La mejor puntuación de exactitud alcanzada con estos hiperparámetros es de aproximadamente 0.911, lo que indica un buen rendimiento del modelo en el conjunto de validación.\n",
    "\n",
    "Ahora que tienes estos mejores hiperparámetros, puedes utilizarlos para ajustar tu modelo de Regresión Logística y luego evaluar su desempeño en el conjunto de prueba para asegurarte de que generaliza bien a datos nuevos y no vistos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cebfdb-2e18-40da-8ffd-4e1003ce9377",
   "metadata": {},
   "source": [
    "# Inicializar un nuevo modelo de Regresión Logística con los mejores hiperparámetros encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6363963-d7ea-4b38-87b2-2b78a2ffdd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = LogisticRegression(max_iter=100000, penalty=grid_search.best_params_['penalty'], \n",
    "                                C=grid_search.best_params_['C'])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento completos (no divididos)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las etiquetas para el conjunto de prueba\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión del modelo en el conjunto de prueba\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Precisión del modelo en el conjunto de prueba:\", accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a12717-861c-421e-9633-586cf647a008",
   "metadata": {},
   "source": [
    "# paso 1: predecir las etiquetas en el conjunto de prueba utilizando el modelo ajustado con los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895db1a-e1d3-48e3-8a63-7485ff04883b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1122e500-1ac1-485f-a01b-37782367235f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Matriz de Confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58790390-e95f-4082-8ac3-0615640c2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por ejemplo, si tus características están en un DataFrame llamado dataframeTrain_clean y tus etiquetas en una Serie llamada y:\n",
    "X = dataframeTrain_clean.drop(columns=['y'])\n",
    "y = dataframeTrain_clean['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89517458-b373-46a2-9f75-4bd17bcae7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que tienes un modelo llamado 'modelo' ya entrenado\n",
    "# y deseas obtener las predicciones para el conjunto de prueba 'X_test'\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3a552-6b1e-4237-8655-9eef21b57999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = dataframeTrain_clean.drop(columns=['y'])\n",
    "y = dataframeTrain_clean['y']\n",
    "# Suponiendo que tienes tus características en X y tus etiquetas en y\n",
    "# Reemplaza X e y con tus propios datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ahora puedes utilizar X_train, X_test, y_train, y y_test para entrenar tu modelo y evaluar su rendimiento.\n",
    "\n",
    "# Aquí se supone que tienes tus predicciones (y_pred) y tus etiquetas verdaderas (y_test).\n",
    "y_pred = model.predict(X_test)\n",
    "# Si ya tienes y_pred y y_test definidos, puedes calcular la matriz de confusión y visualizarla.\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualizar la matriz de confusión utilizando seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f10736-965f-4faa-a0fe-af2f6b194670",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Curva ROC y AUC:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13113065-329b-424a-856e-4e5d2574b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Calcular las probabilidades de las clases positivas\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calcular la tasa de verdaderos positivos y la tasa de falsos positivos\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "# Calcular el área bajo la curva ROC (AUC)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Visualizar la curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab85b3-606e-48c6-b055-d3a5b7556961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Calcular la precisión y el recall\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# Calcular el área bajo la curva PR (AUC)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Visualizar la curva de Precisión-Recall\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva de Precisión-Recall')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08048af4-a429-41bc-a9a8-2a04b57ce28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los coeficientes de la regresión logística\n",
    "coefficients = best_model.coef_[0]\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Visualizar los coeficientes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, coefficients, color='skyblue')\n",
    "plt.xlabel('Coeficiente')\n",
    "plt.ylabel('Característica')\n",
    "plt.title('Coeficientes de Regresión Logística')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c118302-c022-47d1-8cb3-21fbda154a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Obtener la importancia de las características\n",
    "    importances = best_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Visualizar la importancia de las características\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_names[indices], importances[indices], color='green')\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.ylabel('Característica')\n",
    "    plt.title('Importancia de Características (Random Forest)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"El modelo no tiene atributo 'feature_importances_'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ed63e-7bbe-463b-b070-4cde89864f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc36cb3-27d3-4470-a5de-a0ff73490690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c4fe3-778c-4598-b6f0-bedc2353186f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aabe71-d5de-496f-b82d-57af9082b089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
