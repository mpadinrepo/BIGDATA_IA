{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fc2c45-6e08-4583-a44d-7605f9d15b3e",
   "metadata": {},
   "source": [
    "## 1 Preprocesamiento de datos:\r\n",
    "Tratar los valores faltantes: En este caso, las columnas \"Age\", \"Fare\", \"Cabin\" y \"Embarked\" tienen valores faltantes en uno o ambos conjuntos de datos. \n",
    "\n",
    "Podemos decidir cómo tratar estos valores faltantes, ya sea eliminando las filas correspondientes, imputando valores utilizando la media o la mediana, o utilizando técnicas más avanzadas como la imputación por regresión.\n",
    "\n",
    "\r\n",
    "Codificación de variables categóricas: Las variables categóricas como \"Sex\" y \"Embarked\" deben ser codificadas numéricamente antes de alimentarlas al modelo. Esto se puede lograr mediante técnicas como la codificación one-hot (o dummy encoding)\n",
    "\n",
    ".\r\n",
    "Eliminación de columnas no relevantes: Columnas como \"PassengerId\", \"Name\" y \"Ticket\" pueden no ser relevantes para el modelo y se pueden eliminar del conjunto de datos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323b4cd5-3bff-4b48-8d07-a5f873cff4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del conjunto de entrenamiento:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    891 non-null    int64  \n",
      " 1   Pclass      891 non-null    int64  \n",
      " 2   Age         891 non-null    float64\n",
      " 3   SibSp       891 non-null    int64  \n",
      " 4   Parch       891 non-null    int64  \n",
      " 5   Fare        891 non-null    float64\n",
      " 6   Cabin       891 non-null    int64  \n",
      " 7   Sex_male    891 non-null    bool   \n",
      " 8   Embarked_Q  891 non-null    bool   \n",
      " 9   Embarked_S  891 non-null    bool   \n",
      "dtypes: bool(3), float64(2), int64(5)\n",
      "memory usage: 51.5 KB\n",
      "None\n",
      "\n",
      "Información del conjunto de prueba:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Pclass      418 non-null    int64  \n",
      " 1   Age         418 non-null    float64\n",
      " 2   SibSp       418 non-null    int64  \n",
      " 3   Parch       418 non-null    int64  \n",
      " 4   Fare        418 non-null    float64\n",
      " 5   Cabin       418 non-null    int64  \n",
      " 6   Sex_male    418 non-null    bool   \n",
      " 7   Embarked_Q  418 non-null    bool   \n",
      " 8   Embarked_S  418 non-null    bool   \n",
      "dtypes: bool(3), float64(2), int64(4)\n",
      "memory usage: 20.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos del archivo CSV\n",
    "df_train = pd.read_csv('titanic_train.csv')\n",
    "df_test = pd.read_csv('titanic_test.csv')\n",
    "\n",
    "# Eliminar columnas no relevantes para el modelo\n",
    "cols_to_drop = ['PassengerId', 'Name', 'Ticket']\n",
    "df_train = df_train.drop(cols_to_drop, axis=1)\n",
    "df_test = df_test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Tratar valores faltantes\n",
    "# 1. Tratar 'Age': Llenar los valores faltantes con la mediana de 'Age'\n",
    "age_median = df_train['Age'].median()\n",
    "df_train['Age'].fillna(age_median, inplace=True)\n",
    "df_test['Age'].fillna(age_median, inplace=True)\n",
    "\n",
    "# 2. Tratar 'Fare': Llenar el valor faltante con la mediana de 'Fare'\n",
    "fare_median = df_test['Fare'].median()\n",
    "df_test['Fare'].fillna(fare_median, inplace=True)\n",
    "\n",
    "# 3. Tratar 'Cabin': Crear una nueva columna indicando si la cabina está presente o no\n",
    "df_train['Cabin'] = df_train['Cabin'].notnull().astype('int')\n",
    "df_test['Cabin'] = df_test['Cabin'].notnull().astype('int')\n",
    "\n",
    "# 4. Tratar 'Embarked': Llenar los valores faltantes con el valor más común\n",
    "embarked_mode = df_train['Embarked'].mode()[0]\n",
    "df_train['Embarked'].fillna(embarked_mode, inplace=True)\n",
    "\n",
    "# Codificar variables categóricas\n",
    "df_train = pd.get_dummies(df_train, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "df_test = pd.get_dummies(df_test, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Verificar los datos después del preprocesamiento\n",
    "print(\"Información del conjunto de entrenamiento:\")\n",
    "print(df_train.info())\n",
    "\n",
    "print(\"\\nInformación del conjunto de prueba:\")\n",
    "print(df_test.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb55dae-894e-4e92-901e-a41199234e9d",
   "metadata": {},
   "source": [
    "## 2 División de datos:\n",
    "Dividir el conjunto de entrenamiento en datos de entrenamiento y datos de validación. Esto nos permitirá evaluar el rendimiento del modelo antes de probarlo en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65ccaaf-8e15-4160-98e2-6a5246d9ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 712\n",
      "Tamaño del conjunto de validación: 179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir las características (X) y la variable objetivo (y)\n",
    "X = df_train.drop('Survived', axis=1)\n",
    "y = df_train['Survived']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(X_train))\n",
    "print(\"Tamaño del conjunto de validación:\", len(X_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030250ee-cbf9-4dd0-95bc-68d8cf3e6c7a",
   "metadata": {},
   "source": [
    "## 3 Entrenamiento del modelo\n",
    "Utilizar el conjunto de entrenamiento preprocesado para entrenar un modelo de árbol de decisión. Podemos ajustar los hiperparámetros del modelo, como la profundidad máxima del árbol, el criterio de división, etc., utilizando la validación cruzada o técnicas de búsqueda en cuadrícula para encontrar la mejor configuración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb2fb0-2ba0-4f6a-9fbb-d311a2202294",
   "metadata": {},
   "source": [
    "Importar la clase DecisionTreeClassifier de sklearn.tree.\n",
    "Opcionalmente, importar GridSearchCV de sklearn.model_selection si queremos realizar una búsqueda en cuadrícula para ajustar los hiperparámetros.\n",
    "Instanciar un objeto de la clase DecisionTreeClassifier.\n",
    "Opcionalmente, definir un diccionario de hiperparámetros que queremos ajustar.\n",
    "Opcionalmente, definir un objeto GridSearchCV para buscar en la cuadrícula los mejores hiperparámetros.\n",
    "Llamar al método fit del objeto DecisionTreeClassifier (o del objeto GridSearchCV si lo estamos utilizando) con los datos de entrenamiento (X_train y y_train).\n",
    "\n",
    "En este ejemplo, se utiliza GridSearchCV para buscar en la cuadrícula los mejores hiperparámetros. Si no deseas ajustar los hiperparámetros y prefieres utilizar los valores predeterminados, puedes omitir la parte relacionada con param_grid y GridSearchCV, y simplemente entrenar un modelo directamente con DecisionTreeClassifier() y luego llamar al método fit.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e472b068-755b-47ee-85e6-a8f603edcdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Crear un objeto de árbol de decisión\n",
    "best_tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Definir el diccionario de hiperparámetros a ajustar (opcional)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Crear un objeto GridSearchCV para buscar en la cuadrícula los mejores hiperparámetros (opcional)\n",
    "grid_search = GridSearchCV(best_tree_model, param_grid, cv=5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados (opcional)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Utilizar los mejores hiperparámetros para entrenar un nuevo modelo\n",
    "best_tree_model = DecisionTreeClassifier(**best_params)\n",
    "best_tree_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7d543-4829-4467-b760-d71b56c646b3",
   "metadata": {},
   "source": [
    "## 4 Evaluación del modelo:\n",
    "Evaluar el rendimiento del modelo utilizando métricas como la precisión, el recall, el F1-score, y la matriz de confusión en el conjunto de validación. Esto nos permitirá tener una idea de cómo se desempeña el modelo antes de probarlo en el conjunto de prueba.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c10499-653a-4f61-a431-8b2b43a0c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del conjunto de entrenamiento:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    891 non-null    int64  \n",
      " 1   Pclass      891 non-null    int64  \n",
      " 2   Age         891 non-null    float64\n",
      " 3   SibSp       891 non-null    int64  \n",
      " 4   Parch       891 non-null    int64  \n",
      " 5   Fare        891 non-null    float64\n",
      " 6   Cabin       891 non-null    int64  \n",
      " 7   Sex_male    891 non-null    bool   \n",
      " 8   Embarked_Q  891 non-null    bool   \n",
      " 9   Embarked_S  891 non-null    bool   \n",
      "dtypes: bool(3), float64(2), int64(5)\n",
      "memory usage: 51.5 KB\n",
      "None\n",
      "\n",
      "Información del conjunto de prueba:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Pclass      418 non-null    int64  \n",
      " 1   Age         418 non-null    float64\n",
      " 2   SibSp       418 non-null    int64  \n",
      " 3   Parch       418 non-null    int64  \n",
      " 4   Fare        418 non-null    float64\n",
      " 5   Cabin       418 non-null    int64  \n",
      " 6   Sex_male    418 non-null    bool   \n",
      " 7   Embarked_Q  418 non-null    bool   \n",
      " 8   Embarked_S  418 non-null    bool   \n",
      "dtypes: bool(3), float64(2), int64(4)\n",
      "memory usage: 20.9 KB\n",
      "None\n",
      "Tamaño del conjunto de entrenamiento: 712\n",
      "Tamaño del conjunto de validación: 179\n",
      "Mejores hiperparámetros: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}\n",
      "Precision (Precisión): 0.8913043478260869\n",
      "Recall (Recuperación): 0.5942028985507246\n",
      "F1-score (Puntuación F1): 0.7130434782608696\n",
      "Accuracy (Precisión global): 0.8156424581005587\n",
      "Confusion Matrix (Matriz de confusión):\n",
      "[[105   5]\n",
      " [ 28  41]]\n",
      "\n",
      "Conclusiones:\n",
      "El modelo tiene una precisión global satisfactoria.\n",
      "El equilibrio entre precisión y recall del modelo necesita ser mejorado.\n",
      "La puntuación F1 del modelo necesita ser mejorada.\n",
      "El modelo tiende a cometer más falsos negativos que falsos positivos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Cargar los datos del archivo CSV\n",
    "df_train = pd.read_csv('titanic_train.csv')\n",
    "df_test = pd.read_csv('titanic_test.csv')\n",
    "\n",
    "# Eliminar columnas no relevantes para el modelo\n",
    "cols_to_drop = ['PassengerId', 'Name', 'Ticket']\n",
    "df_train = df_train.drop(cols_to_drop, axis=1)\n",
    "df_test = df_test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Tratar valores faltantes\n",
    "# 1. Tratar 'Age': Llenar los valores faltantes con la mediana de 'Age'\n",
    "age_median = df_train['Age'].median()\n",
    "df_train['Age'].fillna(age_median, inplace=True)\n",
    "df_test['Age'].fillna(age_median, inplace=True)\n",
    "\n",
    "# 2. Tratar 'Fare': Llenar el valor faltante con la mediana de 'Fare'\n",
    "fare_median = df_test['Fare'].median()\n",
    "df_test['Fare'].fillna(fare_median, inplace=True)\n",
    "\n",
    "# 3. Tratar 'Cabin': Crear una nueva columna indicando si la cabina está presente o no\n",
    "df_train['Cabin'] = df_train['Cabin'].notnull().astype('int')\n",
    "df_test['Cabin'] = df_test['Cabin'].notnull().astype('int')\n",
    "\n",
    "# 4. Tratar 'Embarked': Llenar los valores faltantes con el valor más común\n",
    "embarked_mode = df_train['Embarked'].mode()[0]\n",
    "df_train['Embarked'].fillna(embarked_mode, inplace=True)\n",
    "\n",
    "# Codificar variables categóricas\n",
    "df_train = pd.get_dummies(df_train, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "df_test = pd.get_dummies(df_test, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Verificar los datos después del preprocesamiento\n",
    "print(\"Información del conjunto de entrenamiento:\")\n",
    "print(df_train.info())\n",
    "\n",
    "print(\"\\nInformación del conjunto de prueba:\")\n",
    "print(df_test.info())\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X = df_train.drop('Survived', axis=1)\n",
    "y = df_train['Survived']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(X_train))\n",
    "print(\"Tamaño del conjunto de validación:\", len(X_val))\n",
    "\n",
    "# Crear un objeto de árbol de decisión\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Definir el diccionario de hiperparámetros a ajustar\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Crear un objeto GridSearchCV para buscar en la cuadrícula los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(tree_model, param_grid, cv=5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Utilizar los mejores hiperparámetros para entrenar un nuevo modelo\n",
    "best_tree_model = DecisionTreeClassifier(**best_params)\n",
    "best_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de validación\n",
    "predictions_val = best_tree_model.predict(X_val)\n",
    "\n",
    "# Calcular las métricas de evaluación\n",
    "precision = precision_score(y_val, predictions_val)\n",
    "recall = recall_score(y_val, predictions_val)\n",
    "f1 = f1_score(y_val, predictions_val)\n",
    "accuracy = accuracy_score(y_val, predictions_val)\n",
    "cm = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "# Imprimir métricas con explicaciones y conclusiones\n",
    "print(\"Precision (Precisión):\", precision)\n",
    "print(\"Recall (Recuperación):\", recall)\n",
    "print(\"F1-score (Puntuación F1):\", f1)\n",
    "print(\"Accuracy (Precisión global):\", accuracy)\n",
    "print(\"Confusion Matrix (Matriz de confusión):\")\n",
    "print(cm)\n",
    "\n",
    "# Conclusiones\n",
    "print(\"\\nConclusiones:\")\n",
    "if accuracy >= 0.8:\n",
    "    print(\"El modelo tiene una precisión global satisfactoria.\")\n",
    "else:\n",
    "    print(\"El modelo necesita mejoras en su rendimiento.\")\n",
    "\n",
    "if precision >= 0.8 and recall >= 0.8:\n",
    "    print(\"El modelo tiene un buen equilibrio entre precisión y recall.\")\n",
    "else:\n",
    "    print(\"El equilibrio entre precisión y recall del modelo necesita ser mejorado.\")\n",
    "\n",
    "if f1 >= 0.8:\n",
    "    print(\"El modelo tiene una puntuación F1 satisfactoria.\")\n",
    "else:\n",
    "    print(\"La puntuación F1 del modelo necesita ser mejorada.\")\n",
    "\n",
    "if cm[0, 1] > cm[1, 0]:\n",
    "    print(\"El modelo tiende a cometer más falsos positivos que falsos negativos.\")\n",
    "elif cm[0, 1] < cm[1, 0]:\n",
    "    print(\"El modelo tiende a cometer más falsos negativos que falsos positivos.\")\n",
    "else:\n",
    "    print(\"El modelo comete errores de manera equilibrada entre las clases.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0144d7-3118-4456-877e-f7294544befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86       110\n",
      "           1       0.89      0.59      0.71        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.84      0.77      0.79       179\n",
      "weighted avg       0.83      0.82      0.81       179\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[105   5]\n",
      " [ 28  41]]\n",
      "\n",
      "Razonamientos Interpretativos:\n",
      "\n",
      "Razonamientos:\n",
      "- El modelo tiene una precisión global del 81.56%, indicando que el 81.56% de las predicciones en el conjunto de prueba son correctas.\n",
      "- La precisión y recall son equilibrados, especialmente para la clase 0 (No sobrevivieron).\n",
      "- La clase 0 (No sobrevivieron) tiene una precisión de 0.89 y un recall de 0.59.\n",
      "- La clase 1 (Sobrevivió) tiene una precisión de 0.79 y un recall de 0.95.\n",
      "- F1-score proporciona un equilibrio entre precisión y recall, siendo 0.71 para la clase 0 y 0.86 para la clase 1.\n",
      "- En la matriz de confusión, los True Positives (TP) y True Negatives (TN) son (41, 105), indicando una buena capacidad del modelo para clasificar ambas clases.\n",
      "- Los False Positives (FP) y False Negatives (FN) son (5, 28), indicando que el modelo cometió 5 falsos positivos y 28 falsos negativos.\n",
      "- Los valores de los razonamientos pueden variar dependiendo de la orientación del problema y los objetivos específicos del negocio.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Obtener el reporte de clasificación\n",
    "classification_rep = classification_report(y_val, predictions_val)\n",
    "\n",
    "# Obtener la matriz de confusión\n",
    "confusion_mat = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print()\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print()\n",
    "\n",
    "# Razonamientos Interpretativos:\n",
    "print(\"Razonamientos Interpretativos:\\n\")\n",
    "print(\"Razonamientos:\")\n",
    "# Calcular y mostrar los razonamientos basados en la matriz de confusión\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "# Precisión global (Accuracy)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"- El modelo tiene una precisión global del {:.2f}%, indicando que el {:.2f}% de las predicciones en el conjunto de prueba son correctas.\".format(accuracy * 100, accuracy * 100))\n",
    "\n",
    "# Precisión y recall\n",
    "precision_0 = tp / (tp + fp)\n",
    "precision_1 = tn / (tn + fn)\n",
    "recall_0 = tp / (tp + fn)\n",
    "recall_1 = tn / (tn + fp)\n",
    "print(\"- La precisión y recall son equilibrados, especialmente para la clase 0 (No sobrevivieron).\")\n",
    "print(\"- La clase 0 (No sobrevivieron) tiene una precisión de {:.2f} y un recall de {:.2f}.\".format(precision_0, recall_0))\n",
    "print(\"- La clase 1 (Sobrevivió) tiene una precisión de {:.2f} y un recall de {:.2f}.\".format(precision_1, recall_1))\n",
    "\n",
    "# F1-score\n",
    "f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0)\n",
    "f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1)\n",
    "print(\"- F1-score proporciona un equilibrio entre precisión y recall, siendo {:.2f} para la clase 0 y {:.2f} para la clase 1.\".format(f1_0, f1_1))\n",
    "\n",
    "# Imprimir razonamientos adicionales\n",
    "print(\"- En la matriz de confusión, los True Positives (TP) y True Negatives (TN) son ({}, {}), indicando una buena capacidad del modelo para clasificar ambas clases.\".format(tp, tn))\n",
    "print(\"- Los False Positives (FP) y False Negatives (FN) son ({}, {}), indicando que el modelo cometió {} falsos positivos y {} falsos negativos.\".format(fp, fn, fp, fn))\n",
    "print(\"- Los valores de los razonamientos pueden variar dependiendo de la orientación del problema y los objetivos específicos del negocio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c411d5e-a0b0-4ee3-9109-738f3b602588",
   "metadata": {},
   "source": [
    "## 5 Pruebas adicionales y ajustes:\r",
    "Realizar pruebas adicionales y ajustes en el modelo según sea necesario para mejorar su rendimiento.\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5951138a-683b-4bb9-b400-526ede908851",
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/graphviz/backend/execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m graph \u001b[38;5;241m=\u001b[39m graphviz\u001b[38;5;241m.\u001b[39mSource(dot_data)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Visualizar el árbol\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitanic_decision_tree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleanup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m graph\u001b[38;5;241m.\u001b[39mview()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Hacer predicciones en el conjunto de validación\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/graphviz/rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[0;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/graphviz/backend/rendering.py:324\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mFileExistsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput file exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mfspath(outfile)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m--> 324\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/graphviz/backend/execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import graphviz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Cargar los datos del archivo CSV\n",
    "df_train = pd.read_csv('titanic_train.csv')\n",
    "df_test = pd.read_csv('titanic_test.csv')\n",
    "\n",
    "# Eliminar columnas no relevantes para el modelo\n",
    "cols_to_drop = ['PassengerId', 'Name', 'Ticket']\n",
    "df_train = df_train.drop(cols_to_drop, axis=1)\n",
    "df_test = df_test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Tratar valores faltantes\n",
    "# 1. Tratar 'Age': Llenar los valores faltantes con la mediana de 'Age'\n",
    "age_median = df_train['Age'].median()\n",
    "df_train['Age'].fillna(age_median, inplace=True)\n",
    "df_test['Age'].fillna(age_median, inplace=True)\n",
    "\n",
    "# 2. Tratar 'Fare': Llenar el valor faltante con la mediana de 'Fare'\n",
    "fare_median = df_test['Fare'].median()\n",
    "df_test['Fare'].fillna(fare_median, inplace=True)\n",
    "\n",
    "# 3. Tratar 'Cabin': Crear una nueva columna indicando si la cabina está presente o no\n",
    "df_train['Cabin'] = df_train['Cabin'].notnull().astype('int')\n",
    "df_test['Cabin'] = df_test['Cabin'].notnull().astype('int')\n",
    "\n",
    "# 4. Tratar 'Embarked': Llenar los valores faltantes con el valor más común\n",
    "embarked_mode = df_train['Embarked'].mode()[0]\n",
    "df_train['Embarked'].fillna(embarked_mode, inplace=True)\n",
    "\n",
    "# Codificar variables categóricas\n",
    "df_train = pd.get_dummies(df_train, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "df_test = pd.get_dummies(df_test, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Definir las características (X) y la variable objetivo (y)\n",
    "X = df_train.drop('Survived', axis=1)\n",
    "y = df_train['Survived']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# Crear un objeto de árbol de decisión y entrenarlo\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Exportar el árbol como un archivo .dot\n",
    "dot_data = export_graphviz(tree_model, out_file=None, \n",
    "                           feature_names=X_train.columns,  \n",
    "                           class_names=['No Sobrevivió', 'Sobrevivió'],\n",
    "                           filled=True, rounded=True, special_characters=True)\n",
    "\n",
    "# Convertir el archivo .dot a un gráfico\n",
    "graph = graphviz.Source(dot_data)\n",
    "\n",
    "# Visualizar el árbol\n",
    "graph.render('titanic_decision_tree', format='png', cleanup=True)\n",
    "graph.view()\n",
    "\n",
    "# Hacer predicciones en el conjunto de validación\n",
    "predictions_val = tree_model.predict(X_val)\n",
    "\n",
    "# Obtener el reporte de clasificación\n",
    "classification_rep = classification_report(y_val, predictions_val)\n",
    "\n",
    "# Obtener la matriz de confusión\n",
    "confusion_mat = confusion_matrix(y_val, predictions_val)\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "print()\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print()\n",
    "\n",
    "# Razonamientos Interpretativos:\n",
    "print(\"Razonamientos Interpretativos:\\n\")\n",
    "print(\"Razonamientos:\")\n",
    "# Calcular y mostrar los razonamientos basados en la matriz de confusión\n",
    "tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "# Precisión global (Accuracy)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"- El modelo tiene una precisión global del {:.2f}%, indicando que el {:.2f}% de las predicciones en el conjunto de prueba son correctas.\".format(accuracy * 100, accuracy * 100))\n",
    "\n",
    "# Precisión y recall\n",
    "precision_0 = tp / (tp + fp)\n",
    "precision_1 = tn / (tn + fn)\n",
    "recall_0 = tp / (tp + fn)\n",
    "recall_1 = tn / (tn + fp)\n",
    "print(\"- La precisión y recall son equilibrados, especialmente para la clase 0 (No sobrevivieron).\")\n",
    "print(\"- La clase 0 (No sobrevivieron) tiene una precisión de {:.2f} y un recall de {:.2f}.\".format(precision_0, recall_0))\n",
    "print(\"- La clase 1 (Sobrevivió) tiene una precisión de {:.2f} y un recall de {:.2f}.\".format(precision_1, recall_1))\n",
    "\n",
    "# F1-score\n",
    "f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0)\n",
    "f1_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1)\n",
    "print(\"- F1-score proporciona un equilibrio entre precisión y recall, siendo {:.2f} para la clase 0 y {:.2f} para la clase 1.\".format(f1_0, f1_1))\n",
    "\n",
    "# Imprimir razonamientos adicionales\n",
    "print(\"- En la matriz de confusión, los True Positives (TP) y True Negatives (TN) son ({}, {}), indicando una buena capacidad del modelo para clasificar ambas clases.\".format(tp, tn))\n",
    "print(\"- Los False Positives (FP) y False Negatives (FN) son ({}, {}), indicando que el modelo cometió {} falsos positivos y {} falsos negativos.\".format(fp, fn, fp, fn))\n",
    "print(\"- Los valores de los razonamientos pueden variar dependiendo de la orientación del problema y los objetivos específicos del negocio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27a41d8-68f9-4f8e-ad4b-c59164b33271",
   "metadata": {},
   "source": [
    "## 6 Predicción en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac93c52-1b69-48de-8067-1452cd07bda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df24e539-3664-45ec-afc8-91e3b0ccfe74",
   "metadata": {},
   "source": [
    "##  7 Evaluación final del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858d5d3-4d28-4279-8582-4911913e472e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a851bf8-38fd-458e-bd7d-c17f6a965d4e",
   "metadata": {},
   "source": [
    "## 8 Iteración y refinamiento \n",
    "Si es necesario, podemos iterar en los pasos anteriores, refinando el preprocesamiento de datos, ajustando el modelo y evaluando su rendimiento hasta que estemos satisfechos con los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec383d85-ae79-49f3-9ba6-34942cc5b2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
